![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red?logo=pytorch)
![Federated Learning](https://img.shields.io/badge/Federated-Learning-green)
![Differential Privacy](https://img.shields.io/badge/Differential-Privacy-purple)
![Streamlit](https://img.shields.io/badge/Dashboard-Streamlit-orange?logo=streamlit)
![Status](https://img.shields.io/badge/Status-Active%20Research-brightgreen)

# ğŸ”¬ Federated Learning for Semiconductor Predictive Maintenance

A **research-grade**, privacy-preserving federated learning framework for predictive maintenance in semiconductor manufacturing. Uses advanced LSTM, Attention-LSTM, and Transformer models on the [SECOM dataset](https://archive.ics.uci.edu/ml/datasets/SECOM) â€” with differential privacy, multiple FL strategies, anomaly detection, and an interactive Streamlit dashboard.

---

## ğŸš€ Key Results

| Model / Strategy | Accuracy | F1-Score | Privacy | Data Sharing |
|-----------------|----------|----------|---------|--------------|
| Centralized     | ~0.76    | ~0.66    | âŒ None  | âŒ Raw data shared |
| FedAvg          | ~0.73    | ~0.63    | âŒ None  | âœ… No sharing |
| FedProx         | ~0.74    | ~0.64    | âŒ None  | âœ… No sharing |
| FedNova         | ~0.73    | ~0.63    | âŒ None  | âœ… No sharing |
| DP-FedAvg       | ~0.70    | ~0.58    | âœ… (Îµ,Î´)-DP | âœ… No sharing |

> **Key Insight**: Federated models achieve **~96% of centralized performance** while keeping all sensor data local. With DP, we add formal privacy guarantees at a modest accuracy cost.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ”¬ FL Semiconductor Framework             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Fab A   â”‚  â”‚  Fab B   â”‚  â”‚  Fab C   â”‚  â† Clients      â”‚
â”‚  â”‚ (Client) â”‚  â”‚ (Client) â”‚  â”‚ (Client) â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚       â”‚              â”‚              â”‚                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚              â”‚              â”‚                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                         â”‚
â”‚         â”‚ Gradient â”‚   â”‚DP Noise  â”‚ â† Privacy Layer        â”‚
â”‚         â”‚ Clipping â”‚   â”‚Injection â”‚                         â”‚
â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚              â”‚              â”‚                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                          â”‚
â”‚         â”‚  FL Aggregation Server â”‚                          â”‚
â”‚         â”‚  FedAvg / FedProx /    â”‚ â† Strategy Layer        â”‚
â”‚         â”‚  FedNova              â”‚                           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                     â”‚                                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚              â”‚ Global Model â”‚                               â”‚
â”‚              â”‚ LSTM / Attn  â”‚ â† Model Layer                â”‚
â”‚              â”‚ Transformer  â”‚                               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                     â”‚                                       â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚         â”‚  ğŸ“Š Dashboard &       â”‚                           â”‚
â”‚         â”‚  Visualization        â”‚ â† Monitoring Layer       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
federated-learning-semiconductor-predictive-maintenance/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml              # Central experiment configuration
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py               # Model registry & factory
â”‚   â”œâ”€â”€ lstm_model.py             # Standard LSTM classifier
â”‚   â”œâ”€â”€ attention_lstm.py         # LSTM + Bahdanau attention
â”‚   â””â”€â”€ transformer_model.py      # Temporal Transformer encoder
â”‚
â”œâ”€â”€ fl_strategies/
â”‚   â”œâ”€â”€ __init__.py               # Strategy registry
â”‚   â”œâ”€â”€ base.py                   # Abstract FederatedStrategy
â”‚   â”œâ”€â”€ fedavg.py                 # FedAvg (McMahan et al., 2017)
â”‚   â”œâ”€â”€ fedprox.py                # FedProx with proximal term
â”‚   â””â”€â”€ fednova.py                # FedNova normalized averaging
â”‚
â”œâ”€â”€ privacy/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dp_fedavg.py              # DP-FedAvg (clipping + noise)
â”‚   â””â”€â”€ privacy_accountant.py     # RDP-based (Îµ,Î´) tracker
â”‚
â”œâ”€â”€ anomaly/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ autoencoder.py            # LSTM autoencoder
â”‚   â””â”€â”€ anomaly_detector.py       # Federated anomaly pipeline
â”‚
â”œâ”€â”€ data_utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ noniid_partition.py       # Dirichlet/label/quantity skew
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py            # Data loading & preprocessing
â”‚   â”œâ”€â”€ metrics.py                # Training & evaluation utilities
â”‚   â””â”€â”€ logger.py                 # Structured JSON experiment logger
â”‚
â”œâ”€â”€ visualization/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ advanced_plots.py         # ROC, t-SNE, confusion matrix, etc.
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                    # Streamlit interactive dashboard
â”‚
â”œâ”€â”€ data/                         # SECOM dataset (not in repo)
â”œâ”€â”€ results/                      # Models, plots, logs
â”‚
â”œâ”€â”€ run_experiment.py             # Config-driven experiment runner
â”œâ”€â”€ secom_preprocess.py           # SECOM data preprocessing
â”œâ”€â”€ make_windows.py               # Sliding window creation
â”œâ”€â”€ centralized_train.py          # Centralized baseline (legacy)
â”œâ”€â”€ federated_train.py            # Federated training (legacy)
â”œâ”€â”€ compare_models.py             # Model comparison (legacy)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Installation & Setup

```bash
# Clone the repository
git clone https://github.com/<your-username>/federated-learning-semiconductor-predictive-maintenance.git
cd federated-learning-semiconductor-predictive-maintenance

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Data Setup

Download the [SECOM dataset](https://archive.ics.uci.edu/ml/datasets/SECOM) and place `secom.data` and `secom_labels.data` in the `data/` directory.

```bash
# Step 1: Preprocess
python secom_preprocess.py

# Step 2: Create windows & client splits
python make_windows.py
```

---

## ğŸš€ Usage

### Run a Full Experiment

```bash
# Default configuration (FedAvg + LSTM)
python run_experiment.py

# Custom configuration
python run_experiment.py --config configs/default.yaml
```

### Configuration Options

Edit `configs/default.yaml` to customize:

```yaml
model:
  type: "lstm"          # lstm | attention_lstm | transformer

federated:
  strategy: "fedavg"    # fedavg | fedprox | fednova
  rounds: 5
  dp:
    enabled: false      # Enable differential privacy
    noise_multiplier: 1.0
    max_grad_norm: 1.0
```

### Launch the Dashboard

```bash
streamlit run dashboard/app.py
```

---

## ğŸ§  Phase-by-Phase Capabilities

### Phase 1 â€” Foundation
- Clean config-driven architecture
- Model registry with factory pattern
- Structured JSON experiment logging

### Phase 2 â€” Advanced FL Strategies
- **FedProx**: Proximal regularization for non-IID resilience
- **FedNova**: Normalized averaging for heterogeneous updates
- **Non-IID Simulation**: Dirichlet, label-skew, quantity-skew partitioning

### Phase 3 â€” Privacy
- **DP-FedAvg**: Per-client gradient clipping + Gaussian noise
- **RDP Accountant**: Formal (Îµ,Î´)-differential privacy tracking

### Phase 4 â€” Advanced Models
- **Attention-LSTM**: Interpretable attention over timesteps
- **Transformer**: Multi-head self-attention for long-range dependencies
- **Anomaly Detection**: Federated LSTM autoencoder

### Phase 5 â€” Dashboard
- **Streamlit Dashboard**: Interactive training monitor, model comparison, privacy tracker
- **Advanced Plots**: ROC curves, confusion matrices, t-SNE, radar charts

---

## ğŸ“Š Key Visualizations

The framework generates:
- Training loss & accuracy curves
- Centralized vs Federated comparison bar charts
- FL strategy convergence comparisons
- Privacy budget (Îµ) accumulation plots
- Client data distribution visualizations
- Confusion matrix heatmaps
- ROC curves with AUC comparison
- t-SNE embedding plots
- Per-client radar charts

---

## ğŸ“š References

| Paper | Topic |
|-------|-------|
| [McMahan et al., 2017](https://arxiv.org/abs/1602.05629) | FedAvg |
| [Li et al., 2020](https://arxiv.org/abs/1812.06127) | FedProx |
| [Wang et al., 2020](https://arxiv.org/abs/2007.07481) | FedNova |
| [McMahan et al., 2018](https://arxiv.org/abs/1710.06963) | DP-FL |
| [Mironov, 2017](https://arxiv.org/abs/1702.07476) | RÃ©nyi DP |

---

## ğŸ” Reproducibility

All experiments are fully reproducible:
- Random seeds fixed via config (`seed: 42`)
- Identical train/test splits across runs
- Full experiment configs logged to JSON
- Environment: Python 3.9+, PyTorch 2.0+

---

## ğŸ“„ License

This project is for research and educational purposes.

---

## ğŸ“– Project Guide

For a comprehensive, easy-to-understand breakdown of the entire project â€” including architecture explanations, interview Q&As, and talking points â€” see **[docs/PROJECT_GUIDE.md](docs/PROJECT_GUIDE.md)**.

---

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or submit a pull request.
